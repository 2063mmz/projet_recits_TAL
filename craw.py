from playwright.sync_api import sync_playwright
from bs4 import BeautifulSoup
import time
import os
import re

def clean_filename(text):
    """æ¸…ç†æ–‡ä»¶åï¼Œå»é™¤éæ³•å­—ç¬¦"""
    # ç§»é™¤æˆ–æ›¿æ¢æ–‡ä»¶åä¸­çš„éæ³•å­—ç¬¦
    text = re.sub(r'[\\/*?:"<>|]', '', text)
    # é™åˆ¶é•¿åº¦
    if len(text) > 100:
        text = text[:100]
    return text.strip()

def extract_links_with_keyword(page, keyword="è”åˆ"):
    """ä»åˆ—è¡¨é¡µæå–åŒ…å«å…³é”®è¯çš„é“¾æ¥"""
    try:
        # è·å–é¡µé¢æ‰€æœ‰é“¾æ¥
        links = []
        elements = page.locator('a[href]').all()
        
        for elem in elements:
            try:
                text = elem.text_content().strip()
                href = elem.get_attribute('href')
                
                # åªé€‰æ‹©åŒ…å«å…³é”®è¯çš„é“¾æ¥
                if keyword in text and href:
                    # æ„å»ºå®Œæ•´URL
                    if href.startswith('/'):
                        full_url = 'https://www.yidaiyilu.gov.cn' + href
                    elif not href.startswith('http'):
                        full_url = 'https://www.yidaiyilu.gov.cn/' + href
                    else:
                        full_url = href
                    
                    links.append({
                        'title': text,
                        'url': full_url
                    })
                    print(f"   æ‰¾åˆ°: {text}")
            except:
                continue
        
        return links
    except Exception as e:
        print(f"âŒ æå–é“¾æ¥å¤±è´¥: {e}")
        return []

def parse_article_content(html):
    """è§£ææ–‡ç« é¡µé¢å†…å®¹"""
    soup = BeautifulSoup(html, 'html.parser')
    
    # æå–æ ‡é¢˜
    title = ""
    title_candidates = [
        soup.find('h1'),
        soup.find('h2'),
        soup.find('div', class_='article-title'),
        soup.find('title')
    ]
    
    for candidate in title_candidates:
        if candidate:
            title = candidate.get_text().strip()
            if title and len(title) > 5:
                break
    
    # ç§»é™¤ä¸éœ€è¦çš„å…ƒç´ 
    unwanted_tags = ['script', 'style', 'nav', 'footer', 'header', 'iframe', 'noscript']
    for tag in unwanted_tags:
        for element in soup.find_all(tag):
            element.decompose()
    
    # æŸ¥æ‰¾ä¸»è¦å†…å®¹åŒºåŸŸ
    main_content = (
        soup.find('article') or 
        soup.find('main') or
        soup.find('div', class_=re.compile(r'content|article|main', re.I)) or
        soup.find('body')
    )
    
    if not main_content:
        return {'title': title, 'content': 'æœªèƒ½æå–åˆ°å†…å®¹'}
    
    # æå–æ®µè½
    paragraphs = main_content.find_all(['p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6'])
    
    # æ”¶é›†æ–‡æœ¬å¹¶å»é‡
    seen_texts = set()
    content_lines = []
    
    # å¯¼èˆªå…³é”®è¯
    navigation_keywords = [
        'é¦–é¡µ', 'èµ„è®¯', 'æ”¿ç­–', 'é¡¹ç›®', 'æ•°æ®', 'æœåŠ¡',
        'ç®€ä½“ç‰ˆ', 'ç¹é«”ç‰ˆ', 'English', 'FranÃ§ais',
        'æ— éšœç¢', 'å‹æƒ…é“¾æ¥', 'å…³äºæˆ‘ä»¬', 'å®˜ç½‘åŠ¨æ€',
        'å¯¼èˆª', 'è¯­è¨€', 'ç½‘ç«™å¯¼èˆª'
    ]
    
    for para in paragraphs:
        text = para.get_text().strip()
        
        # è¿‡æ»¤æ¡ä»¶
        if not text or len(text) < 15:
            continue
        if text in seen_texts:
            continue
        
        # è¿‡æ»¤å¯¼èˆªæ–‡æœ¬
        if len(text) < 50 and any(keyword in text for keyword in navigation_keywords):
            continue
        
        seen_texts.add(text)
        content_lines.append(text)
    
    text_content = '\n\n'.join(content_lines)
    
    return {
        'title': title,
        'content': text_content
    }

def save_article(data, output_dir='articles'):
    """ä¿å­˜æ–‡ç« åˆ°txtæ–‡ä»¶"""
    try:
        # åˆ›å»ºè¾“å‡ºç›®å½•
        if not os.path.exists(output_dir):
            os.makedirs(output_dir)
        
        # ç”Ÿæˆæ–‡ä»¶å
        filename = clean_filename(data['title'])
        if not filename:
            filename = f"article_{int(time.time())}"
        
        filepath = os.path.join(output_dir, f"{filename}.txt")
        
        # ä¿å­˜æ–‡ä»¶
        with open(filepath, 'w', encoding='utf-8') as file:
            file.write(f"æ ‡é¢˜: {data['title']}\n\n")
            file.write("=" * 80 + "\n\n")
            file.write(data['content'])
        
        print(f"   âœ… å·²ä¿å­˜: {filepath}")
        return True
    except Exception as e:
        print(f"   âŒ ä¿å­˜å¤±è´¥: {e}")
        return False

def check_next_page(page):
    """æ£€æŸ¥æ˜¯å¦æœ‰ä¸‹ä¸€é¡µ"""
    try:
        # æŸ¥æ‰¾ä¸‹ä¸€é¡µæŒ‰é’®
        next_button = page.locator('a:has-text("ä¸‹ä¸€é¡µ"), a:has-text(">"), .next-page').first
        if next_button.is_visible():
            return True
        
        # æ£€æŸ¥åˆ†é¡µæ•°å­—
        current_page_elem = page.locator('.current, .active, [class*="current"], [class*="active"]').first
        if current_page_elem.count() > 0:
            return True
        
        return False
    except:
        return False

def go_to_next_page(page):
    """ç¿»åˆ°ä¸‹ä¸€é¡µ"""
    try:
        # å°è¯•ç‚¹å‡»ä¸‹ä¸€é¡µæŒ‰é’®
        next_selectors = [
            'a:has-text("ä¸‹ä¸€é¡µ")',
            'a:has-text(">")',
            '.next-page',
            'a[rel="next"]'
        ]
        
        for selector in next_selectors:
            try:
                next_button = page.locator(selector).first
                if next_button.is_visible():
                    next_button.click()
                    time.sleep(2)
                    return True
            except:
                continue
        
        # å¦‚æœæŒ‰é’®ç‚¹å‡»å¤±è´¥ï¼Œå°è¯•é€šè¿‡URLç¿»é¡µ
        current_url = page.url
        if 'page=' in current_url:
            match = re.search(r'page=(\d+)', current_url)
            if match:
                current_page = int(match.group(1))
                next_page = current_page + 1
                next_url = re.sub(r'page=\d+', f'page={next_page}', current_url)
                page.goto(next_url)
                time.sleep(2)
                return True
        
        return False
    except Exception as e:
        print(f"   âš ï¸ ç¿»é¡µå¤±è´¥: {e}")
        return False

def crawl_list_page(page, keyword="è”åˆ", output_dir='articles'):
    """çˆ¬å–åˆ—è¡¨é¡µä¸­åŒ…å«å…³é”®è¯çš„æ–‡ç« """
    try:
        # ç­‰å¾…é¡µé¢åŠ è½½
        page.wait_for_load_state('networkidle')
        time.sleep(2)
        
        print(f"\nğŸ“‹ å½“å‰é¡µé¢: {page.url}")
        
        # æå–åŒ…å«å…³é”®è¯çš„é“¾æ¥
        print(f"ğŸ” æœç´¢åŒ…å«'{keyword}'çš„é“¾æ¥...")
        links = extract_links_with_keyword(page, keyword)
        
        if not links:
            print(f"   âš ï¸ æœªæ‰¾åˆ°åŒ…å«'{keyword}'çš„é“¾æ¥")
            return 0
        
        print(f"   æ‰¾åˆ° {len(links)} ä¸ªåŒ…å«'{keyword}'çš„é“¾æ¥\n")
        
        # è®¿é—®æ¯ä¸ªé“¾æ¥å¹¶ä¿å­˜å†…å®¹
        success_count = 0
        for i, link_info in enumerate(links, 1):
            print(f"ğŸ“„ [{i}/{len(links)}] æ­£åœ¨å¤„ç†: {link_info['title']}")
            
            try:
                # è®¿é—®è¯¦æƒ…é¡µ
                page.goto(link_info['url'])
                page.wait_for_load_state('networkidle')
                time.sleep(1)
                
                # è·å–å¹¶è§£æå†…å®¹
                html = page.content()
                data = parse_article_content(html)
                
                # ä¿å­˜æ–‡ç« 
                if save_article(data, output_dir):
                    success_count += 1
                
                # è¿”å›åˆ—è¡¨é¡µ
                page.go_back()
                page.wait_for_load_state('networkidle')
                time.sleep(1)
                
            except Exception as e:
                print(f"   âŒ å¤„ç†å¤±è´¥: {e}")
                # å°è¯•è¿”å›åˆ—è¡¨é¡µ
                try:
                    page.go_back()
                except:
                    pass
        
        return success_count
    except Exception as e:
        print(f"âŒ çˆ¬å–åˆ—è¡¨é¡µå¤±è´¥: {e}")
        return 0

def main():
    """ä¸»å‡½æ•°"""
    base_url = 'https://www.yidaiyilu.gov.cn/list/w/sdbwj?page=1'
    keyword = "è”åˆ"
    output_dir = 'articles'
    
    print("=" * 80)
    print("ğŸš€ ä¸€å¸¦ä¸€è·¯ç½‘ç«™æ–‡ç« çˆ¬è™«")
    print("=" * 80)
    print(f"å…³é”®è¯: {keyword}")
    print(f"è¾“å‡ºç›®å½•: {output_dir}")
    print(f"èµ·å§‹é¡µ: {base_url}\n")
    
    try:
        with sync_playwright() as p:
            print("ğŸŒ å¯åŠ¨æµè§ˆå™¨...")
            browser = p.firefox.launch(headless=False)
            page = browser.new_page()
            
            # è®¿é—®èµ·å§‹é¡µ
            print(f"ğŸ“¡ è®¿é—®èµ·å§‹é¡µé¢...")
            page.goto(base_url)
            
            total_articles = 0
            page_num = 1
            
            # å¾ªç¯å¤„ç†æ¯ä¸€é¡µ
            while True:
                print(f"\n{'='*80}")
                print(f"ğŸ“– ç¬¬ {page_num} é¡µ")
                print(f"{'='*80}")
                
                # çˆ¬å–å½“å‰é¡µ
                count = crawl_list_page(page, keyword, output_dir)
                total_articles += count
                
                print(f"\nâœ… ç¬¬ {page_num} é¡µå®Œæˆï¼ŒæˆåŠŸä¿å­˜ {count} ç¯‡æ–‡ç« ")
                
                # æ£€æŸ¥æ˜¯å¦æœ‰ä¸‹ä¸€é¡µ
                print("\nğŸ”„ æ£€æŸ¥ä¸‹ä¸€é¡µ...")
                if not go_to_next_page(page):
                    print("ğŸ“Œ å·²åˆ°è¾¾æœ€åä¸€é¡µ")
                    break
                
                page_num += 1
                time.sleep(2)
            
            browser.close()
            
            print(f"\n{'='*80}")
            print(f"âœ… çˆ¬å–å®Œæˆï¼")
            print(f"   æ€»é¡µæ•°: {page_num}")
            print(f"   æˆåŠŸä¿å­˜: {total_articles} ç¯‡æ–‡ç« ")
            print(f"   ä¿å­˜ä½ç½®: {output_dir}/")
            print(f"{'='*80}")
            
    except Exception as e:
        print(f"\nâŒ ç¨‹åºå¼‚å¸¸: {e}")
        import traceback
        traceback.print_exc()

if __name__ == '__main__':
    main()
