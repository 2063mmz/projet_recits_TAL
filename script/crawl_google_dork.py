"""
ä½¿ç”¨Google Dorkæœç´¢ä¸€å¸¦ä¸€è·¯ç›¸å…³å†…å®¹
é€šè¿‡Googleæœç´¢å„ä¸ªç½‘ç«™ï¼Œè€Œä¸æ˜¯ç›´æ¥è®¿é—®
"""
from playwright.sync_api import sync_playwright
from bs4 import BeautifulSoup
import time
import os
import re
from urllib.parse import quote

def read_target_websites(file_path='links.txt'):
    """è¯»å–ç›®æ ‡ç½‘ç«™åˆ—è¡¨"""
    websites = []
    with open(file_path, 'r', encoding='utf-8') as f:
        for line in f:
            line = line.strip()
            # è·³è¿‡ç©ºè¡Œã€æ³¨é‡Šå’Œæ ‡é¢˜è¡Œ
            if not line or line.startswith('#') or line.startswith('=') or '|' not in line:
                continue
            
            try:
                parts = [p.strip() for p in line.split('|')]
                if len(parts) >= 4:
                    url, country, keyword, file_prefix = parts[0], parts[1], parts[2], parts[3]
                    websites.append({
                        'url': url,
                        'country': country,
                        'keyword': keyword,
                        'file_prefix': file_prefix
                    })
            except Exception as e:
                print(f"âš ï¸ è·³è¿‡æ— æ•ˆè¡Œ: {line}")
    
    return websites

def build_google_query(keyword, site_url):
    """æ„å»ºGoogleæœç´¢æŸ¥è¯¢"""
    # æ¸…ç†URLï¼Œç§»é™¤åè®®å‰ç¼€
    clean_url = site_url.replace('https://', '').replace('http://', '').rstrip('/')
    
    # æ„å»ºæŸ¥è¯¢
    query = f"{keyword} site:{clean_url}"
    return query

def extract_results_from_page(page):
    """ä»å½“å‰é¡µé¢æå–Googleæœç´¢ç»“æœ"""
    results = []
    html = page.content()
    soup = BeautifulSoup(html, 'html.parser')
    
    # å°è¯•å¤šç§é€‰æ‹©å™¨æ¥æ‰¾åˆ°æœç´¢ç»“æœ
    # Googleæœç´¢ç»“æœçš„ç»“æ„å¯èƒ½å˜åŒ–ï¼Œæ‰€ä»¥ä½¿ç”¨å¤šç§æ–¹æ³•
    
    # æ–¹æ³•1: æŸ¥æ‰¾æ‰€æœ‰åŒ…å«é“¾æ¥å’Œæ ‡é¢˜çš„div
    search_divs = soup.find_all('div', class_='g')
    
    # æ–¹æ³•2: å¦‚æœæ–¹æ³•1æ²¡æœ‰ç»“æœï¼Œå°è¯•æŸ¥æ‰¾æ‰€æœ‰ä¸»è¦æœç´¢ç»“æœ
    if not search_divs:
        search_divs = soup.find_all('div', attrs={'data-sokoban-container': True})
    
    # æ–¹æ³•3: æŸ¥æ‰¾æ‰€æœ‰åŒ…å«h3æ ‡é¢˜çš„çˆ¶div
    if not search_divs:
        h3_tags = soup.find_all('h3')
        search_divs = [h3.find_parent('div') for h3 in h3_tags if h3.find_parent('div')]
    
    print(f"      æ‰¾åˆ° {len(search_divs)} ä¸ªå€™é€‰ç»“æœ")
    
    for div in search_divs:
        try:
            # æŸ¥æ‰¾é“¾æ¥
            a_tag = div.find('a', href=True)
            if not a_tag:
                continue
            
            url = a_tag.get('href', '')
            
            # è·³è¿‡Googleå†…éƒ¨é“¾æ¥å’Œæ— æ•ˆé“¾æ¥
            if not url or url.startswith('/search') or 'google.com/search' in url:
                continue
            
            # å¦‚æœé“¾æ¥æ˜¯ç›¸å¯¹è·¯å¾„ï¼Œè·³è¿‡
            if not url.startswith('http'):
                continue
            
            # æŸ¥æ‰¾æ ‡é¢˜ï¼ˆh3æ ‡ç­¾ï¼‰
            h3_tag = div.find('h3')
            title = h3_tag.get_text(strip=True) if h3_tag else 'No Title'
            
            # è·³è¿‡æ²¡æœ‰æ ‡é¢˜çš„ç»“æœ
            if not title or title == 'No Title':
                continue
            
            # æŸ¥æ‰¾æè¿°ï¼ˆå¤šç§å¯èƒ½çš„classï¼‰
            description = ''
            desc_classes = ['VwiC3b', 'IsZvec', 'lEBKkf']
            for desc_class in desc_classes:
                desc_tag = div.find('div', class_=desc_class)
                if desc_tag:
                    description = desc_tag.get_text(strip=True)
                    break
            
            # å¦‚æœè¿˜æ²¡æ‰¾åˆ°æè¿°ï¼Œå°è¯•æŸ¥æ‰¾span
            if not description:
                desc_span = div.find('span', class_='aCOpRe')
                if desc_span:
                    description = desc_span.get_text(strip=True)
            
            # æå–æ—¥æœŸ
            date = ''
            date_match = re.search(r'(20[0-2]\d[-/å¹´]\d{1,2}[-/æœˆ]\d{1,2}[æ—¥]?)', 
                                   title + description + url)
            if date_match:
                date = date_match.group(0)
            
            # åˆ¤æ–­æ–‡ä»¶ç±»å‹
            file_type = 'HTML'
            url_lower = url.lower()
            if url_lower.endswith('.pdf') or '[PDF]' in title:
                file_type = 'PDF'
            elif url_lower.endswith(('.doc', '.docx')):
                file_type = 'DOC'
            
            results.append({
                'title': title[:200],
                'url': url,
                'description': description[:300],
                'type': file_type,
                'date': date
            })
            
        except Exception as e:
            continue
    
    return results

def google_search_with_pagination(page, query, max_pages=10, is_first_search=False):
    """åœ¨Googleä¸Šæœç´¢å¹¶è‡ªåŠ¨ç¿»é¡µæå–ç»“æœ"""
    print(f"   ğŸ” æœç´¢æŸ¥è¯¢: {query}")
    print(f"   ğŸ“„ æœ€å¤šç¿»é¡µ: {max_pages} é¡µ")
    
    # æ„å»ºGoogleæœç´¢URL
    encoded_query = quote(query)
    google_url = f"https://www.google.com/search?q={encoded_query}&num=100"
    
    all_results = []
    seen_urls = set()
    
    try:
        # ç¬¬ä¸€æ¬¡è®¿é—®
        print(f"   ğŸ“¡ è®¿é—®Google...")
        page.goto(google_url, wait_until='domcontentloaded', timeout=30000)
        
        # æ¥å—cookiesï¼ˆå¦‚æœæœ‰å¼¹çª—ï¼‰
        try:
            accept_buttons = page.locator('button:has-text("Accept all"), button:has-text("Accept"), button:has-text("å…¨éƒ¨æ¥å—"), button:has-text("åŒæ„"), button:has-text("æ¥å—")')
            if accept_buttons.count() > 0:
                print(f"   ğŸª æ¥å—cookies...")
                accept_buttons.first.click(timeout=3000)
                time.sleep(2)
        except:
            pass
        
        # åªåœ¨ç¬¬ä¸€æ¬¡æœç´¢æ—¶ç­‰å¾…æ›´é•¿æ—¶é—´ï¼Œè®©ç”¨æˆ·æœ‰æ—¶é—´å®ŒæˆäººæœºéªŒè¯
        if is_first_search:
            print(f"   â³ ç­‰å¾…15ç§’ï¼ˆè¯·åœ¨æ­¤æœŸé—´å®ŒæˆäººæœºéªŒè¯ï¼‰...")
            time.sleep(15)
        else:
            time.sleep(3)
        
        # å¼€å§‹ç¿»é¡µ
        current_page = 1
        
        while current_page <= max_pages:
            print(f"\n   ğŸ“„ ç¬¬ {current_page}/{max_pages} é¡µ")
            
            # ç­‰å¾…é¡µé¢åŠ è½½
            time.sleep(2)
            
            # æå–å½“å‰é¡µç»“æœ
            page_results = extract_results_from_page(page)
            
            # å»é‡å¹¶æ·»åŠ ç»“æœ
            new_count = 0
            for result in page_results:
                if result['url'] not in seen_urls:
                    seen_urls.add(result['url'])
                    all_results.append(result)
                    new_count += 1
            
            print(f"      âœ… æœ¬é¡µæ–°å¢ {new_count} ä¸ªç»“æœï¼ˆæ€»è®¡: {len(all_results)}ï¼‰")
            
            # å¦‚æœæœ¬é¡µæ²¡æœ‰æ–°ç»“æœï¼Œå¯èƒ½æ˜¯åˆ°åº•äº†
            if new_count == 0 and current_page > 1:
                print(f"      âš ï¸ æœ¬é¡µæ— æ–°ç»“æœï¼Œåœæ­¢ç¿»é¡µ")
                break
            
            # æŸ¥æ‰¾"ä¸‹ä¸€é¡µ"æŒ‰é’®
            if current_page < max_pages:
                try:
                    # å¤šç§"ä¸‹ä¸€é¡µ"æŒ‰é’®çš„é€‰æ‹©å™¨
                    next_selectors = [
                        'a#pnnext',
                        'a[aria-label="Next page"]',
                        'a[aria-label="ä¸‹ä¸€é¡µ"]',
                        'a:has-text("Next")',
                        'a:has-text("ä¸‹ä¸€é¡µ")',
                        'span:has-text("Next")',
                    ]
                    
                    next_button = None
                    for selector in next_selectors:
                        try:
                            btn = page.locator(selector).first
                            if btn.count() > 0 and btn.is_visible(timeout=2000):
                                next_button = btn
                                break
                        except:
                            continue
                    
                    if next_button:
                        print(f"      ğŸ”„ ç‚¹å‡»ä¸‹ä¸€é¡µ...")
                        next_button.click()
                        current_page += 1
                        time.sleep(3)  # ç­‰å¾…æ–°é¡µé¢åŠ è½½
                    else:
                        print(f"      â„¹ï¸ æ²¡æœ‰æ‰¾åˆ°ä¸‹ä¸€é¡µæŒ‰é’®")
                        break
                        
                except Exception as e:
                    print(f"      âš ï¸ ç¿»é¡µå¤±è´¥: {e}")
                    break
            else:
                current_page += 1
        
        print(f"\n   âœ… æœç´¢å®Œæˆï¼Œå…± {current_page-1} é¡µï¼Œ{len(all_results)} ä¸ªç»“æœ")
        return all_results
        
    except Exception as e:
        print(f"   âŒ Googleæœç´¢å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return all_results  # è¿”å›å·²ç»æ”¶é›†åˆ°çš„ç»“æœ

def crawl_with_google_dork(page, website_info, output_dir='google_dork_results', max_pages=10, is_first_search=False):
    """ä½¿ç”¨Google Dorkçˆ¬å–å•ä¸ªç½‘ç«™"""
    url = website_info['url']
    country = website_info['country']
    keyword = website_info['keyword']
    file_prefix = website_info['file_prefix']
    
    print(f"\n{'='*80}")
    print(f"ğŸŒ æ­£åœ¨æœç´¢: {country}")
    print(f"ğŸ”— ç›®æ ‡ç½‘ç«™: {url}")
    print(f"ğŸ” å…³é”®è¯: {keyword}")
    print(f"{'='*80}")
    
    # æ„å»ºGoogleæŸ¥è¯¢
    query = build_google_query(keyword, url)
    
    # æ‰§è¡Œæœç´¢ï¼ˆå¸¦ç¿»é¡µï¼‰
    results = google_search_with_pagination(page, query, max_pages, is_first_search)
    
    print(f"\n   âœ… æ€»è®¡æ‰¾åˆ° {len(results)} ä¸ªç»“æœ")
    
    # ä¿å­˜ç»“æœ
    if results:
        output_file = os.path.join(output_dir, f"{file_prefix}_links.txt")
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(f"{'='*80}\n")
            f.write(f"{country} - ä¸€å¸¦ä¸€è·¯ç›¸å…³é“¾æ¥ (Googleæœç´¢ç»“æœ)\n")
            f.write(f"ç›®æ ‡ç½‘ç«™: {url}\n")
            f.write(f"æœç´¢æŸ¥è¯¢: {query}\n")
            f.write(f"çˆ¬å–æ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"{'='*80}\n\n")
            
            for i, link in enumerate(results, 1):
                f.write(f"{i}. {link['title']}\n")
                f.write(f"   URL: {link['url']}\n")
                f.write(f"   ç±»å‹: {link['type']}")
                if link['date']:
                    f.write(f" | æ—¥æœŸ: {link['date']}")
                f.write(f"\n")
                if link['description']:
                    f.write(f"   æè¿°: {link['description']}\n")
                f.write(f"\n")
        
        print(f"   ğŸ’¾ å·²ä¿å­˜åˆ°: {output_file}")
    
    return len(results)

def generate_summary(output_dir='google_dork_results'):
    """ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š"""
    print(f"\n{'='*80}")
    print("ğŸ“Š ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š...")
    print(f"{'='*80}")
    
    summary_file = os.path.join(output_dir, 'summary_report.txt')
    
    # ç»Ÿè®¡æ‰€æœ‰æ–‡ä»¶
    link_files = [f for f in os.listdir(output_dir) if f.endswith('_links.txt')]
    
    total_links = 0
    results = []
    
    for file_name in sorted(link_files):
        file_path = os.path.join(output_dir, file_name)
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
            # ç»Ÿè®¡é“¾æ¥æ•°é‡
            count = content.count('URL:')
            total_links += count
            
            # æå–å›½å®¶å
            country_match = re.search(r'^(.+?) - ä¸€å¸¦ä¸€è·¯ç›¸å…³é“¾æ¥', content, re.MULTILINE)
            country = country_match.group(1) if country_match else file_name
            
            results.append({
                'file': file_name,
                'country': country,
                'count': count
            })
    
    # å†™å…¥æ±‡æ€»æŠ¥å‘Š
    with open(summary_file, 'w', encoding='utf-8') as f:
        f.write(f"{'='*80}\n")
        f.write("ä¸€å¸¦ä¸€è·¯å¤šå›½ç½‘ç«™çˆ¬å– - Google Dork æ±‡æ€»æŠ¥å‘Š\n")
        f.write(f"ç”Ÿæˆæ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"{'='*80}\n\n")
        
        f.write(f"æ€»ç½‘ç«™æ•°: {len(link_files)}\n")
        f.write(f"æ€»é“¾æ¥æ•°: {total_links}\n\n")
        
        f.write(f"{'='*80}\n")
        f.write("å„ç½‘ç«™è¯¦ç»†ç»Ÿè®¡:\n")
        f.write(f"{'='*80}\n\n")
        
        for i, result in enumerate(results, 1):
            f.write(f"{i}. {result['country']}\n")
            f.write(f"   æ–‡ä»¶: {result['file']}\n")
            f.write(f"   é“¾æ¥æ•°: {result['count']}\n\n")
        
        f.write(f"{'='*80}\n")
        f.write("å®Œæˆï¼\n")
        f.write(f"{'='*80}\n")
    
    print(f"âœ… æ±‡æ€»æŠ¥å‘Šå·²ä¿å­˜: {summary_file}")
    print(f"\nğŸ“Š ç»Ÿè®¡ç»“æœ:")
    print(f"   æ€»ç½‘ç«™æ•°: {len(link_files)}")
    print(f"   æ€»é“¾æ¥æ•°: {total_links}")

def main():
    """ä¸»å‡½æ•°"""
    print("="*80)
    print("ğŸš€ ä¸€å¸¦ä¸€è·¯å¤šå›½ç½‘ç«™é“¾æ¥çˆ¬å–å·¥å…· (Google Dorkæ–¹æ³•)")
    print("="*80)
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = 'google_dork_results'
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)
        print(f"ğŸ“ åˆ›å»ºè¾“å‡ºç›®å½•: {output_dir}")
    
    # è¯»å–ç›®æ ‡ç½‘ç«™
    print("\nğŸ“– è¯»å–ç›®æ ‡ç½‘ç«™åˆ—è¡¨...")
    websites = read_target_websites('links.txt')[13:]
    print(f"âœ… æ‰¾åˆ° {len(websites)} ä¸ªç›®æ ‡ç½‘ç«™\n")
    
    # å¼€å§‹çˆ¬å–
    try:
        with sync_playwright() as p:
            print("ğŸŒ å¯åŠ¨æµè§ˆå™¨...")
            browser = p.firefox.launch(headless=False)
            page = browser.new_page()
            
            success_count = 0
            total_links = 0
            
            for i, website in enumerate(websites, 1):
                print(f"\nè¿›åº¦: [{i}/{len(websites)}]")
                # åªæœ‰ç¬¬ä¸€ä¸ªç½‘ç«™éœ€è¦ç­‰å¾…15ç§’å®ŒæˆäººæœºéªŒè¯
                is_first = (i == 1)
                links_count = crawl_with_google_dork(page, website, output_dir, max_pages=10, is_first_search=is_first)
                if links_count > 0:
                    success_count += 1
                    total_links += links_count
                
                # æ¯ä¸ªæœç´¢ä¹‹é—´éœ€è¦å»¶è¿Ÿï¼Œé¿å…è¢«Googleé™åˆ¶
                if i < len(websites):
                    delay = 10  # Googleæœç´¢éœ€è¦æ›´é•¿çš„å»¶è¿Ÿ
                    print(f"\n   â³ ç­‰å¾… {delay} ç§’åæœç´¢ä¸‹ä¸€ä¸ªç½‘ç«™...")
                    time.sleep(delay)
            
            browser.close()
            
            # ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š
            print(f"\n{'='*80}")
            print("ğŸ“Š çˆ¬å–å®Œæˆï¼")
            print(f"{'='*80}")
            print(f"æˆåŠŸæœç´¢: {success_count}/{len(websites)} ä¸ªç½‘ç«™")
            print(f"æ€»é“¾æ¥æ•°: {total_links}")
            print(f"{'='*80}")
            
            generate_summary(output_dir)
            
    except Exception as e:
        print(f"\nâŒ ç¨‹åºå¼‚å¸¸: {e}")
        import traceback
        traceback.print_exc()

if __name__ == '__main__':
    main()

