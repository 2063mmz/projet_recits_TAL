#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è¡ŒåŠ¨å…ƒåˆ†æ (Actant Analysis)
åŸºäºå™äº‹å­¦ç†è®ºï¼Œåˆ†ææ–‡æœ¬ä¸­çš„è¡ŒåŠ¨è€…ã€è¡ŒåŠ¨å’Œå…³ç³»
"""

import json
import re
from pathlib import Path
from typing import List, Dict, Tuple, Set
from collections import defaultdict, Counter
import pandas as pd
import numpy as np

import plotly.express as px

# å°è¯•å¯¼å…¥spaCyï¼ˆç”¨äºNERå’Œä¾å­˜å¥æ³•åˆ†æï¼‰
try:
    import spacy
    SPACY_AVAILABLE = True
    try:
        nlp_en = spacy.load("en_core_web_sm")
    except:
        nlp_en = None
        print("âš ï¸  è‹±æ–‡spaCyæ¨¡å‹æœªå®‰è£…ï¼Œè¿è¡Œ: python -m spacy download en_core_web_sm")
    try:
        nlp_zh = spacy.load("zh_core_web_sm")
    except:
        nlp_zh = None
        print("âš ï¸  ä¸­æ–‡spaCyæ¨¡å‹æœªå®‰è£…ï¼Œè¿è¡Œ: python -m spacy download zh_core_web_sm")
except ImportError:
    SPACY_AVAILABLE = False
    print("âš ï¸  spaCyæœªå®‰è£…ï¼Œå°†ä½¿ç”¨åŸºäºè§„åˆ™çš„æ–¹æ³•")

# é…ç½®è·¯å¾„
CORPUS_DIR = Path(__file__).parent.parent / "corpus"
ARTICLES_TXT_DIR = CORPUS_DIR / "articles_txt"
OUTPUT_DIR = Path(__file__).parent.parent / "actant_results"
OUTPUT_DIR.mkdir(exist_ok=True)

# å›½å®¶æ˜ å°„æ–‡ä»¶
COUNTRY_MAPPING_FILE = CORPUS_DIR / "country_mapping.json"

# è¡ŒåŠ¨å…ƒç±»å‹ï¼ˆåŸºäºGreimasçš„å…­å…ƒæ¨¡å‹ï¼‰
ACTANT_TYPES = {
    'Subject': 'ä¸»ä½“ï¼ˆè¡ŒåŠ¨è€…ï¼‰',
    'Object': 'å®¢ä½“ï¼ˆç›®æ ‡/å¯¹è±¡ï¼‰',
    'Sender': 'å‘é€è€…ï¼ˆåŠ¨æœºæ¥æºï¼‰',
    'Receiver': 'æ¥æ”¶è€…ï¼ˆå—ç›Šè€…ï¼‰',
    'Helper': 'è¾…åŠ©è€…ï¼ˆå¸®åŠ©è€…ï¼‰',
    'Opponent': 'åå¯¹è€…ï¼ˆé˜»ç¢è€…ï¼‰'
}

# å¸¸è§è¡ŒåŠ¨åŠ¨è¯ï¼ˆä¸­è‹±æ–‡ï¼‰
ACTION_VERBS = {
    'en': {
        'cooperation': ['cooperate', 'collaborate', 'partnership', 'joint', 'together'],
        'construction': ['build', 'construct', 'develop', 'establish', 'create'],
        'trade': ['trade', 'export', 'import', 'commerce', 'business'],
        'investment': ['invest', 'fund', 'finance', 'capital'],
        'communication': ['communicate', 'exchange', 'dialogue', 'discuss'],
        'support': ['support', 'assist', 'help', 'aid', 'promote'],
        'oppose': ['oppose', 'resist', 'challenge', 'conflict']
    },
    'zh': {
        'cooperation': ['åˆä½œ', 'åä½œ', 'ä¼™ä¼´', 'å…±åŒ', 'è”åˆ'],
        'construction': ['å»ºè®¾', 'æ„å»º', 'å‘å±•', 'å»ºç«‹', 'åˆ›å»º'],
        'trade': ['è´¸æ˜“', 'å‡ºå£', 'è¿›å£', 'å•†ä¸š', 'ç»è´¸'],
        'investment': ['æŠ•èµ„', 'èµ„é‡‘', 'èèµ„', 'èµ„æœ¬'],
        'communication': ['æ²Ÿé€š', 'äº¤æµ', 'å¯¹è¯', 'è®¨è®º'],
        'support': ['æ”¯æŒ', 'æ´åŠ©', 'å¸®åŠ©', 'ä¿ƒè¿›'],
        'oppose': ['åå¯¹', 'æŠµåˆ¶', 'å†²çª', 'æŒ‘æˆ˜']
    }
}


def clean_text(text: str) -> str:
    """æ¸…ç†æ–‡æœ¬"""
    if not text:
        return ""
    # ç§»é™¤HTMLæ ‡ç­¾
    text = re.sub(r'<[^>]+>', '', text)
    # ç§»é™¤URL
    text = re.sub(r'http[s]?://[^\s]+', '', text)
    # ç§»é™¤è¿‡å¤šçš„ç©ºç™½
    text = re.sub(r'\s+', ' ', text)
    return text.strip()


def detect_language(text: str) -> str:
    """æ£€æµ‹æ–‡æœ¬è¯­è¨€"""
    chinese_chars = sum(1 for c in text if '\u4e00' <= c <= '\u9fff')
    total_chars = len([c for c in text if c.isalnum() or '\u4e00' <= c <= '\u9fff'])
    if total_chars == 0:
        return "unknown"
    chinese_ratio = chinese_chars / total_chars if total_chars > 0 else 0
    return "zh" if chinese_ratio > 0.3 else "en"


def extract_entities_rule_based(text: str, lang: str) -> Dict[str, List[str]]:
    """
    åŸºäºè§„åˆ™æå–å®ä½“ï¼ˆå½“spaCyä¸å¯ç”¨æ—¶ä½¿ç”¨ï¼‰
    ä¼˜åŒ–ç‰ˆæœ¬ï¼šé¿å…æå–ä¸å®Œæ•´çš„å®ä½“
    """
    entities = {
        'countries': [],
        'organizations': [],
        'persons': [],
        'projects': []
    }
    
    # å®Œæ•´çš„å›½å®¶åç§°åˆ—è¡¨ï¼ˆä¸­è‹±æ–‡ï¼‰
    countries = {
        'China', 'Chinese', 'ä¸­å›½', 'ä¸­åäººæ°‘å…±å’Œå›½',
        'Russia', 'Russian', 'ä¿„ç½—æ–¯',
        'Kazakhstan', 'å“ˆè¨å…‹æ–¯å¦',
        'Indonesia', 'å°åº¦å°¼è¥¿äºš',
        'Egypt', 'åŸƒåŠ',
        'Ethiopia', 'åŸƒå¡ä¿„æ¯”äºš',
        'Nigeria', 'å°¼æ—¥åˆ©äºš',
        'Mongolia', 'è’™å¤',
        'Serbia', 'å¡å°”ç»´äºš',
        'Uzbekistan', 'ä¹Œå…¹åˆ«å…‹æ–¯å¦',
        'Morocco', 'æ‘©æ´›å“¥',
        'Tanzania', 'å¦æ¡‘å°¼äºš',
        'Uganda', 'ä¹Œå¹²è¾¾',
        'South Africa', 'å—é',
        'Kenya', 'è‚¯å°¼äºš',
        'Nepal', 'å°¼æ³Šå°”',
        'Greece', 'å¸Œè…Š',
        'Pakistan', 'å·´åŸºæ–¯å¦',
        'Bangladesh', 'å­ŸåŠ æ‹‰å›½',
        'Myanmar', 'ç¼…ç”¸',
        'Laos', 'è€æŒ',
        'Cambodia', 'æŸ¬åŸ”å¯¨',
        'Thailand', 'æ³°å›½',
        'Malaysia', 'é©¬æ¥è¥¿äºš',
        'Singapore', 'æ–°åŠ å¡',
        'Philippines', 'è²å¾‹å®¾',
        'Vietnam', 'è¶Šå—'
    }
    
    # å¸¸è§äººåï¼ˆç‰¹åˆ«æ˜¯ä¸­æ–‡äººåï¼‰
    common_persons_zh = {
        'ä¹ è¿‘å¹³', 'æå…‹å¼º', 'ç‹æ¯…', 'å¼ éª', 'éƒ‘å’Œ', 'é©¬å¯Â·æ³¢ç½—',
        'äºçº¢', 'é©¬å“ˆæ‹‰', 'é©¬å“ˆç‰¹', 'å·´æ‹‰å‰'
    }
    
    # éœ€è¦è¿‡æ»¤çš„ä¸å®Œæ•´å®ä½“ï¼ˆå•å­—ã€ä¸å®Œæ•´ç¼©å†™ç­‰ï¼‰
    invalid_entities = {
        'ä¸­', 'å', 'å›½', 'é˜¿', 'å°¼', 'ä¿„', 'å°', 'åŸƒ', 'è’™', 'å¡', 'ä¹Œ', 'æ‘©', 'å¦', 'ä¹Œå¹²', 'å—é',
        'ä¸­é˜¿', 'ä¸­å°¼', 'ä¸­ä¿„', 'ä¸­å°', 'ä¸­åŸƒ', 'ä¸­è’™', 'ä¸­å¡', 'ä¸­ä¹Œ', 'ä¸­æ‘©', 'ä¸­å¦',
        'å', 'å›½', 'å®¶', 'äºº', 'æ°‘', 'æ”¿', 'åºœ', 'éƒ¨', 'é•¿', 'å¤§', 'ä½¿', 'ä¸»', 'å¸­'
    }
    
    # æå–å›½å®¶ï¼ˆä½¿ç”¨å®Œæ•´åŒ¹é…ï¼Œé¿å…éƒ¨åˆ†åŒ¹é…ï¼‰
    for country in countries:
        # ä½¿ç”¨å•è¯è¾¹ç•Œæˆ–ä¸­æ–‡è¾¹ç•Œæ¥åŒ¹é…
        if lang == 'zh':
            # ä¸­æ–‡ï¼šç¡®ä¿æ˜¯å›½å®¶åç§°çš„å®Œæ•´å‡ºç°
            pattern = re.escape(country)
            if re.search(pattern, text):
                entities['countries'].append(country)
        else:
            # è‹±æ–‡ï¼šä½¿ç”¨å•è¯è¾¹ç•Œ
            pattern = r'\b' + re.escape(country) + r'\b'
            if re.search(pattern, text, re.IGNORECASE):
                entities['countries'].append(country)
    
    # æå–äººåï¼ˆä¸­æ–‡ï¼‰
    if lang == 'zh':
        # å…ˆæå–å·²çŸ¥çš„å¸¸è§äººå
        for person in common_persons_zh:
            if person in text:
                entities['persons'].append(person)
        
        # ä¸­æ–‡äººåæ¨¡å¼ï¼šå¸¸è§å§“æ° + 1-2ä¸ªæ±‰å­—ï¼ˆæ›´é€šç”¨çš„æ¨¡å¼ï¼‰
        # å¸¸è§å§“æ°åˆ—è¡¨ï¼ˆå‰50ä¸ªå¸¸è§å§“æ°ï¼‰
        common_surnames = ['ç‹', 'æ', 'å¼ ', 'åˆ˜', 'é™ˆ', 'æ¨', 'èµµ', 'é»„', 'å‘¨', 'å´', 
                          'å¾', 'å­™', 'èƒ¡', 'æœ±', 'é«˜', 'æ—', 'ä½•', 'éƒ­', 'é©¬', 'ç½—',
                          'æ¢', 'å®‹', 'éƒ‘', 'è°¢', 'éŸ©', 'å”', 'å†¯', 'äº', 'è‘£', 'è§',
                          'ç¨‹', 'æ›¹', 'è¢', 'é‚“', 'è®¸', 'å‚…', 'æ²ˆ', 'æ›¾', 'å½­', 'å•',
                          'è‹', 'å¢', 'è’‹', 'è”¡', 'è´¾', 'ä¸', 'é­', 'è–›', 'å¶', 'é˜',
                          'ä½™', 'æ½˜', 'æœ', 'æˆ´', 'å¤', 'é”º', 'æ±ª', 'ç”°', 'ä»»', 'å§œ',
                          'èŒƒ', 'æ–¹', 'çŸ³', 'å§š', 'è°­', 'å»–', 'é‚¹', 'ç†Š', 'é‡‘', 'é™†',
                          'éƒ', 'å­”', 'ç™½', 'å´”', 'åº·', 'æ¯›', 'é‚±', 'ç§¦', 'æ±Ÿ', 'å²',
                          'é¡¾', 'ä¾¯', 'é‚µ', 'å­Ÿ', 'é¾™', 'ä¸‡', 'æ®µ', 'é›·', 'é’±', 'æ±¤',
                          'å°¹', 'é»', 'æ˜“', 'å¸¸', 'æ­¦', 'ä¹”', 'è´º', 'èµ–', 'é¾š', 'æ–‡',
                          'ä¹ ', 'äº', 'é©¬', 'å·´']
        
        # æ„å»ºäººåæ¨¡å¼ï¼šå§“æ° + 1-2ä¸ªæ±‰å­—ï¼Œå‰åæœ‰è¾¹ç•Œ
        surname_pattern = '|'.join(re.escape(s) for s in common_surnames)
        chinese_name_pattern = rf'(?:{surname_pattern})[\u4e00-\u9fff]{{1,2}}(?![^\u4e00-\u9fff\s])'
        name_matches = re.findall(chinese_name_pattern, text)
        # è¿‡æ»¤ï¼šåªä¿ç•™2-4ä¸ªå­—ç¬¦çš„äººå
        name_matches = [m for m in name_matches if 2 <= len(m) <= 4]
        entities['persons'].extend(name_matches)
    
    # ç»„ç»‡åç§°æ¨¡å¼ï¼ˆæ”¹è¿›ç‰ˆï¼Œé¿å…åŒ¹é…ä¸å®Œæ•´çš„å®ä½“ï¼‰
    org_patterns = {
        'en': [
            r'\b[A-Z][a-z]+ (?:Ministry|Department|Organization|Institution|Bank|Fund|Committee|Embassy)\b',
            r'\b(?:UN|UNESCO|WTO|IMF|World Bank|AIIB|BRICS|ASEAN)\b',
            r'\bBelt and Road\b',
            r'\bOne Belt One Road\b'
        ],
        'zh': [
            # å®Œæ•´çš„ç»„ç»‡åç§°ï¼Œè‡³å°‘3ä¸ªå­—ç¬¦
            r'[^ã€‚ï¼Œï¼›ï¼šï¼ï¼Ÿ\s]{3,15}(?:éƒ¨|å§”å‘˜ä¼š|ç»„ç»‡|æœºæ„|é“¶è¡Œ|åŸºé‡‘|è®ºå›|ä½¿é¦†|å¤§ä½¿é¦†)',
            r'ä¸€å¸¦ä¸€è·¯',
            r'ä¸ç»¸ä¹‹è·¯',
            # æ”¹è¿›ï¼šç¡®ä¿æ˜¯å®Œæ•´çš„åˆä½œ/è®ºå›åç§°ï¼Œé¿å…"ä¸­é˜¿"ã€"ä¸­å°¼"ç­‰
            r'(?:ä¸­å›½|ä¸­åäººæ°‘å…±å’Œå›½)[^ã€‚ï¼Œï¼›ï¼šï¼ï¼Ÿ\s]{2,10}(?:åˆä½œ|è®ºå›|å³°ä¼š|ç»„ç»‡|æœºæ„)',
            r'[^ã€‚ï¼Œï¼›ï¼šï¼ï¼Ÿ\s]{3,12}(?:å›½é™…åˆä½œ|é«˜å³°è®ºå›|åˆä½œè®ºå›)'
        ]
    }
    
    # é¡¹ç›®åç§°æ¨¡å¼
    project_patterns = {
        'en': [
            r'\b[A-Z][a-z]+ (?:Project|Initiative|Program|Plan|Agreement|Corridor|Belt|Road)\b',
            r'\b(?:Economic|Trade|Infrastructure) (?:Corridor|Zone|Belt)\b'
        ],
        'zh': [
            # è‡³å°‘3ä¸ªå­—ç¬¦çš„é¡¹ç›®åç§°
            r'[^ã€‚ï¼Œï¼›ï¼šï¼ï¼Ÿ\s]{3,20}(?:é¡¹ç›®|å€¡è®®|è®¡åˆ’|åè®®|èµ°å»Š|ç»æµå¸¦|é“è·¯|æ¸¯å£)',
            r'[^ã€‚ï¼Œï¼›ï¼šï¼ï¼Ÿ\s]{3,15}(?:åˆä½œé¡¹ç›®|åˆä½œè®¡åˆ’|åˆä½œåè®®)'
        ]
    }
    
    # æå–ç»„ç»‡
    for pattern in org_patterns.get(lang, []):
        matches = re.findall(pattern, text, re.IGNORECASE)
        entities['organizations'].extend(matches)
    
    # æå–é¡¹ç›®
    for pattern in project_patterns.get(lang, []):
        matches = re.findall(pattern, text, re.IGNORECASE)
        entities['projects'].extend(matches)
    
    # åå¤„ç†ï¼šè¿‡æ»¤ä¸åˆç†çš„å®ä½“
    for key in entities:
        filtered = []
        for entity in entities[key]:
            entity = entity.strip()
            # è¿‡æ»¤æ¡ä»¶ï¼š
            # 1. é•¿åº¦è‡³å°‘2ä¸ªå­—ç¬¦ï¼ˆä¸­æ–‡ï¼‰æˆ–3ä¸ªå­—ç¬¦ï¼ˆè‹±æ–‡ï¼‰
            min_len = 2 if lang == 'zh' else 3
            if len(entity) < min_len:
                continue
            # 2. ä¸åœ¨æ— æ•ˆå®ä½“åˆ—è¡¨ä¸­
            if entity in invalid_entities:
                continue
            # 3. ä¸åŒ…å«çº¯æ•°å­—
            if entity.isdigit():
                continue
            # 4. ä¸åŒ…å«ç‰¹æ®Šå­—ç¬¦ï¼ˆé™¤äº†å¸¸è§æ ‡ç‚¹ï¼‰
            if re.match(r'^[^\w\u4e00-\u9fff]+$', entity):
                continue
            filtered.append(entity)
        
        # å»é‡å¹¶æ’åº
        entities[key] = sorted(list(set(filtered)))
    
    return entities


def extract_entities_spacy(text: str, lang: str) -> Dict[str, List[str]]:
    """ä½¿ç”¨spaCyæå–å®ä½“ï¼ˆå¸¦åå¤„ç†è¿‡æ»¤ï¼‰"""
    if lang == 'zh' and nlp_zh:
        doc = nlp_zh(text[:1000000])  # é™åˆ¶é•¿åº¦
    elif lang == 'en' and nlp_en:
        doc = nlp_en(text[:1000000])
    else:
        return extract_entities_rule_based(text, lang)
    
    entities = {
        'countries': [],
        'organizations': [],
        'persons': [],
        'projects': []
    }
    
    # éœ€è¦è¿‡æ»¤çš„ä¸å®Œæ•´å®ä½“
    invalid_entities = {
        'ä¸­', 'å', 'å›½', 'é˜¿', 'å°¼', 'ä¿„', 'å°', 'åŸƒ', 'è’™', 'å¡', 'ä¹Œ', 'æ‘©', 'å¦',
        'ä¸­é˜¿', 'ä¸­å°¼', 'ä¸­ä¿„', 'ä¸­å°', 'ä¸­åŸƒ', 'ä¸­è’™', 'ä¸­å¡', 'ä¸­ä¹Œ', 'ä¸­æ‘©', 'ä¸­å¦',
        'å', 'å›½', 'å®¶', 'äºº', 'æ°‘', 'æ”¿', 'åºœ', 'éƒ¨', 'é•¿', 'å¤§', 'ä½¿', 'ä¸»', 'å¸­'
    }
    
    for ent in doc.ents:
        ent_text = ent.text.strip()
        
        # è¿‡æ»¤ä¸åˆç†çš„å®ä½“
        if len(ent_text) < 2:  # è‡³å°‘2ä¸ªå­—ç¬¦
            continue
        if ent_text in invalid_entities:
            continue
        if ent_text.isdigit():
            continue
        
        if ent.label_ in ['GPE', 'LOC']:  # åœ°ç†æ”¿æ²»å®ä½“/ä½ç½®
            entities['countries'].append(ent_text)
        elif ent.label_ == 'ORG':  # ç»„ç»‡
            entities['organizations'].append(ent_text)
        elif ent.label_ == 'PERSON':  # äººç‰©
            entities['persons'].append(ent_text)
    
    # å»é‡å¹¶æ’åº
    for key in entities:
        entities[key] = sorted(list(set(entities[key])))
    
    return entities


def extract_actions(text: str, lang: str) -> List[Dict]:
    """æå–è¡ŒåŠ¨"""
    actions = []
    verbs = ACTION_VERBS.get(lang, ACTION_VERBS['en'])
    
    # ç®€åŒ–çš„è¡ŒåŠ¨æå–ï¼šæŸ¥æ‰¾åŒ…å«è¡ŒåŠ¨åŠ¨è¯çš„å¥å­
    sentences = re.split(r'[ã€‚ï¼ï¼Ÿ.!?]', text)
    
    for sentence in sentences:
        sentence = sentence.strip()
        if len(sentence) < 10:
            continue
        
        for action_type, action_words in verbs.items():
            for word in action_words:
                if word.lower() in sentence.lower():
                    actions.append({
                        'type': action_type,
                        'sentence': sentence[:200],  # é™åˆ¶é•¿åº¦
                        'keyword': word
                    })
                    break
    
    return actions


def extract_actant_relations(text: str, entities: Dict, actions: List[Dict], lang: str) -> List[Dict]:
    """æå–è¡ŒåŠ¨å…ƒå…³ç³»"""
    relations = []
    
    # ç®€åŒ–çš„å…³ç³»æå–ï¼šåœ¨åŒä¸€å¥å­ä¸­å‡ºç°çš„å®ä½“å’Œè¡ŒåŠ¨
    sentences = re.split(r'[ã€‚ï¼ï¼Ÿ.!?]', text)
    
    for sentence in sentences:
        sentence = sentence.strip()
        if len(sentence) < 10:
            continue
        
        # æ£€æŸ¥å¥å­ä¸­æ˜¯å¦åŒ…å«å®ä½“å’Œè¡ŒåŠ¨
        sentence_entities = []
        for entity_type, entity_list in entities.items():
            for entity in entity_list:
                if entity in sentence:
                    sentence_entities.append((entity_type, entity))
        
        sentence_actions = [a for a in actions if a['keyword'] in sentence.lower()]
        
        # å¦‚æœå¥å­ä¸­æœ‰å®ä½“å’Œè¡ŒåŠ¨ï¼Œåˆ›å»ºå…³ç³»
        if sentence_entities and sentence_actions:
            for entity_type, entity in sentence_entities:
                for action in sentence_actions:
                    relations.append({
                        'actant': entity,
                        'actant_type': entity_type,
                        'action': action['type'],
                        'sentence': sentence[:200]
                    })
    
    return relations


def load_country_mapping() -> Dict[str, str]:
    """åŠ è½½å›½å®¶æ˜ å°„æ–‡ä»¶"""
    try:
        with open(COUNTRY_MAPPING_FILE, 'r', encoding='utf-8') as f:
            data = json.load(f)
        return data.get("country_mapping", {})
    except:
        return {}


def get_country_from_filename(filename: str, country_mapping: Dict[str, str]) -> str:
    """ä»æ–‡ä»¶åæå–å›½å®¶ä¿¡æ¯"""
    # æ–‡ä»¶åæ ¼å¼: {src_file}_{hash}_{title}.txt
    # ä¾‹å¦‚: china_fmprc_links.txt_1f4fe6cc01f2c5b2__ä¸€å¸¦ä¸€è·¯_æ—¥æ–°æœˆå¼‚_ä¸­å¸Œåˆä½œåªäº‰æœå¤•_ä¸­åäººæ°‘å…±å’Œå›½å¤–äº¤éƒ¨.txt
    
    # æŸ¥æ‰¾åŒ¹é…çš„src_file
    for src_file, country in country_mapping.items():
        if filename.startswith(src_file):
            return country
    
    # å¦‚æœæ‰¾ä¸åˆ°ï¼Œå°è¯•ä»æ–‡ä»¶åæ¨æ–­
    filename_lower = filename.lower()
    if 'china' in filename_lower:
        return 'China'
    elif 'russia' in filename_lower:
        return 'Russia'
    elif 'kazakhstan' in filename_lower:
        return 'Kazakhstan'
    elif 'indonesia' in filename_lower:
        return 'Indonesia'
    elif 'egypt' in filename_lower:
        return 'Egypt'
    elif 'ethiopia' in filename_lower:
        return 'Ethiopia'
    elif 'nigeria' in filename_lower:
        return 'Nigeria'
    elif 'mongolia' in filename_lower:
        return 'Mongolia'
    elif 'serbia' in filename_lower:
        return 'Serbia'
    elif 'uzbekistan' in filename_lower:
        return 'Uzbekistan'
    elif 'morocco' in filename_lower:
        return 'Morocco'
    elif 'tanzania' in filename_lower:
        return 'Tanzania'
    elif 'uganda' in filename_lower:
        return 'Uganda'
    elif 'south_africa' in filename_lower or 'southafrica' in filename_lower:
        return 'South_Africa'
    elif 'kenya' in filename_lower:
        return 'Kenya'
    
    return 'Unknown'


def load_documents_from_txt_dir(txt_dir: Path) -> Tuple[List[str], List[Dict]]:
    """
    ä» articles_txt ç›®å½•åŠ è½½æ‰€æœ‰ txt æ–‡ä»¶
    è·³è¿‡å‰8è¡Œå…ƒæ•°æ®ï¼Œä»ç¬¬9è¡Œå¼€å§‹æå–æ–‡æœ¬å†…å®¹
    """
    texts = []
    metadata = []
    
    print(f"ğŸ“– æ­£åœ¨ä»ç›®å½•åŠ è½½æ–‡æ¡£: {txt_dir}")
    
    # åŠ è½½å›½å®¶æ˜ å°„
    country_mapping = load_country_mapping()
    
    # è·å–æ‰€æœ‰txtæ–‡ä»¶
    txt_files = list(txt_dir.glob("*.txt"))
    print(f"   æ‰¾åˆ° {len(txt_files)} ä¸ªtxtæ–‡ä»¶")
    
    skipped_count = 0
    
    for txt_file in txt_files:
        try:
            with open(txt_file, 'r', encoding='utf-8') as f:
                lines = f.readlines()
            
            # è·³è¿‡å‰8è¡Œå…ƒæ•°æ®ï¼Œä»ç¬¬9è¡Œå¼€å§‹ï¼ˆç´¢å¼•8ï¼‰
            if len(lines) < 9:
                skipped_count += 1
                continue
            
            # æå–å…ƒæ•°æ®ï¼ˆå‰8è¡Œï¼‰
            meta_dict = {}
            for i in range(min(8, len(lines))):
                line = lines[i].strip()
                if ':' in line:
                    key, value = line.split(':', 1)
                    meta_dict[key.strip()] = value.strip()
            
            # ä»ç¬¬9è¡Œå¼€å§‹æå–æ–‡æœ¬ï¼ˆè·³è¿‡ç©ºè¡Œï¼‰
            text_lines = []
            start_idx = 8  # ç¬¬9è¡Œï¼ˆç´¢å¼•8ï¼‰
            
            # è·³è¿‡å¯èƒ½çš„ç©ºè¡Œ
            while start_idx < len(lines) and not lines[start_idx].strip():
                start_idx += 1
            
            # æå–æ–‡æœ¬å†…å®¹
            for i in range(start_idx, len(lines)):
                line = lines[i].strip()
                if line:
                    text_lines.append(line)
            
            text = '\n'.join(text_lines)
            
            # æ¸…ç†æ–‡æœ¬
            text = clean_text(text)
            
            # åªå¤„ç†æœ‰æœ‰æ•ˆæ–‡æœ¬çš„æ–‡æ¡£
            if text and len(text) > 100:  # è‡³å°‘100ä¸ªå­—ç¬¦
                # ä»æ–‡ä»¶åæå–å›½å®¶
                country = get_country_from_filename(txt_file.name, country_mapping)
                
                texts.append(text)
                metadata.append({
                    'index': len(texts) - 1,
                    'src_file': txt_file.name,
                    'country': country,
                    'title': meta_dict.get('title', txt_file.stem),
                    'date': meta_dict.get('date', ''),
                    'url': meta_dict.get('final_url', meta_dict.get('seed_url', ''))
                })
            else:
                skipped_count += 1
        except Exception as e:
            print(f"âš ï¸  å¤„ç†æ–‡ä»¶ {txt_file.name} æ—¶å‡ºé”™: {e}")
            skipped_count += 1
            continue
    
    print(f"âœ… æˆåŠŸåŠ è½½ {len(texts)} ä¸ªæ–‡æ¡£")
    if skipped_count > 0:
        print(f"âš ï¸  è·³è¿‡äº† {skipped_count} ä¸ªæ— æ•ˆæˆ–ä½è´¨é‡æ–‡æ¡£")
    return texts, metadata


def analyze_actants(texts: List[str], metadata: List[Dict], output_dir: Path):
    """åˆ†æè¡ŒåŠ¨å…ƒ"""
    print("\nğŸ” æ­£åœ¨åˆ†æè¡ŒåŠ¨å…ƒ...")
    
    all_entities = defaultdict(list)
    all_actions = []
    all_relations = []
    
    for i, (text, meta) in enumerate(zip(texts, metadata)):
        if i % 50 == 0:
            print(f"   å¤„ç†è¿›åº¦: {i}/{len(texts)}")
        
        lang = detect_language(text)
        
        # æå–å®ä½“
        if SPACY_AVAILABLE:
            entities = extract_entities_spacy(text, lang)
        else:
            entities = extract_entities_rule_based(text, lang)
        
        # æå–è¡ŒåŠ¨
        actions = extract_actions(text, lang)
        
        # æå–å…³ç³»
        relations = extract_actant_relations(text, entities, actions, lang)
        
        # æ·»åŠ å…ƒæ•°æ®
        for entity_type, entity_list in entities.items():
            for entity in entity_list:
                all_entities[entity_type].append({
                    'entity': entity,
                    'country': meta.get('country', ''),
                    'title': meta.get('title', ''),
                    'date': meta.get('date', '')
                })
        
        for action in actions:
            action['country'] = meta.get('country', '')
            action['title'] = meta.get('title', '')
            all_actions.append(action)
        
        for relation in relations:
            relation['country'] = meta.get('country', '')
            relation['title'] = meta.get('title', '')
            all_relations.append(relation)
    
    print(f"âœ… æå–å®Œæˆ:")
    print(f"   å®ä½“: {sum(len(v) for v in all_entities.values())} ä¸ª")
    print(f"   è¡ŒåŠ¨: {len(all_actions)} ä¸ª")
    print(f"   å…³ç³»: {len(all_relations)} ä¸ª")
    
    return all_entities, all_actions, all_relations




def visualize_actant_statistics(entities: Dict, actions: List[Dict], relations: List[Dict], 
                                metadata: List[Dict], output_dir: Path):
    """å¯è§†åŒ–è¡ŒåŠ¨å…ƒç»Ÿè®¡ï¼ˆåªä¿ç•™æ ¸å¿ƒç»Ÿè®¡ï¼‰"""
    print("\nğŸ“Š æ­£åœ¨ç”Ÿæˆç»Ÿè®¡å›¾è¡¨...")
    
    # 1. æ ¸å¿ƒå®ä½“é¢‘ç‡ç»Ÿè®¡ï¼ˆåªä¿ç•™æœ€é‡è¦çš„ç±»å‹ï¼‰
    important_types = ['countries', 'organizations', 'persons']
    
    for entity_type in important_types:
        if entity_type not in entities or len(entities[entity_type]) == 0:
            continue
        
        entity_counts = Counter([e['entity'] for e in entities[entity_type]])
        top_entities = entity_counts.most_common(20)
        
        if len(top_entities) == 0:
            continue
        
        df = pd.DataFrame(top_entities, columns=['Entity', 'Count'])
        
        fig = px.bar(
            df,
            x='Count',
            y='Entity',
            orientation='h',
            title=f"{ACTANT_TYPES.get(entity_type, entity_type)} - Top 20",
            labels={'Count': 'å‡ºç°æ¬¡æ•°', 'Entity': 'å®ä½“'}
        )
        fig.update_layout(
            height=600, 
            yaxis={'categoryorder': 'total ascending'},
            xaxis_title='å‡ºç°æ¬¡æ•°',
            yaxis_title='å®ä½“'
        )
        fig_file = output_dir / f"entity_frequency_{entity_type}.html"
        fig.write_html(str(fig_file))
        print(f"âœ… {entity_type}é¢‘ç‡å›¾å·²ä¿å­˜")
    
    # 2. è¡ŒåŠ¨ç±»å‹åˆ†å¸ƒï¼ˆç®€åŒ–ç‰ˆï¼Œä½¿ç”¨æ¡å½¢å›¾ï¼‰
    action_counts = Counter([a['type'] for a in actions])
    if len(action_counts) > 0:
        df = pd.DataFrame(list(action_counts.items()), columns=['Action', 'Count'])
        df = df.sort_values('Count', ascending=True)
        
        fig = px.bar(
            df,
            x='Count',
            y='Action',
            orientation='h',
            title="è¡ŒåŠ¨ç±»å‹åˆ†å¸ƒ",
            labels={'Count': 'å‡ºç°æ¬¡æ•°', 'Action': 'è¡ŒåŠ¨ç±»å‹'}
        )
        fig.update_layout(
            height=400,
            yaxis={'categoryorder': 'total ascending'}
        )
        fig_file = output_dir / "action_distribution.html"
        fig.write_html(str(fig_file))
        print(f"âœ… è¡ŒåŠ¨ç±»å‹åˆ†å¸ƒå›¾å·²ä¿å­˜")


def save_actant_results(entities: Dict, actions: List[Dict], relations: List[Dict], 
                       output_dir: Path):
    """ä¿å­˜è¡ŒåŠ¨å…ƒåˆ†æç»“æœï¼ˆHTMLæ ¼å¼ï¼‰"""
    print("\nğŸ’¾ æ­£åœ¨ä¿å­˜ç»“æœ...")
    
    html_content = """
<!DOCTYPE html>
<html>
<head>
    <meta charset="UTF-8">
    <title>è¡ŒåŠ¨å…ƒåˆ†æç»“æœ</title>
    <style>
        body { font-family: Arial, sans-serif; margin: 20px; background-color: #f5f5f5; }
        .section { background: white; margin: 20px 0; padding: 20px; border-radius: 8px; box-shadow: 0 2px 4px rgba(0,0,0,0.1); }
        h1 { color: #2c3e50; }
        h2 { color: #34495e; border-bottom: 2px solid #3498db; padding-bottom: 10px; }
        table { width: 100%; border-collapse: collapse; margin: 15px 0; }
        th, td { padding: 10px; text-align: left; border-bottom: 1px solid #ddd; }
        th { background-color: #3498db; color: white; }
        tr:hover { background-color: #f5f5f5; }
        .entity-type { color: #e74c3c; font-weight: bold; }
        .count { color: #27ae60; font-weight: bold; }
    </style>
</head>
<body>
    <h1>è¡ŒåŠ¨å…ƒåˆ†æç»“æœ</h1>
"""
    
    # å®ä½“ç»Ÿè®¡
    html_content += """
    <div class="section">
        <h2>å®ä½“ç»Ÿè®¡</h2>
"""
    
    for entity_type, entity_list in entities.items():
        entity_counts = Counter([e['entity'] for e in entity_list])
        top_entities = entity_counts.most_common(20)
        
        html_content += f"""
        <h3>{ACTANT_TYPES.get(entity_type, entity_type)}</h3>
        <table>
            <tr><th>æ’å</th><th>å®ä½“</th><th>å‡ºç°æ¬¡æ•°</th></tr>
"""
        for rank, (entity, count) in enumerate(top_entities, 1):
            html_content += f"<tr><td>{rank}</td><td>{entity}</td><td class='count'>{count}</td></tr>\n"
        
        html_content += "</table>\n"
    
    html_content += "</div>\n"
    
    # è¡ŒåŠ¨ç»Ÿè®¡
    html_content += """
    <div class="section">
        <h2>è¡ŒåŠ¨ç±»å‹ç»Ÿè®¡</h2>
        <table>
            <tr><th>è¡ŒåŠ¨ç±»å‹</th><th>å‡ºç°æ¬¡æ•°</th></tr>
"""
    action_counts = Counter([a['type'] for a in actions])
    for action_type, count in action_counts.most_common():
        html_content += f"<tr><td>{action_type}</td><td class='count'>{count}</td></tr>\n"
    html_content += "</table>\n</div>\n"
    
    # å…³ç³»ç¤ºä¾‹
    html_content += """
    <div class="section">
        <h2>è¡ŒåŠ¨å…ƒå…³ç³»ç¤ºä¾‹</h2>
        <table>
            <tr><th>è¡ŒåŠ¨è€…</th><th>è¡ŒåŠ¨</th><th>å›½å®¶</th><th>ä¸Šä¸‹æ–‡</th></tr>
"""
    for relation in relations[:50]:  # åªæ˜¾ç¤ºå‰50ä¸ª
        html_content += f"""
        <tr>
            <td>{relation['actant']}</td>
            <td>{relation['action']}</td>
            <td>{relation.get('country', 'Unknown')}</td>
            <td>{relation['sentence'][:100]}...</td>
        </tr>
"""
    html_content += "</table>\n</div>\n"
    
    html_content += """
</body>
</html>
"""
    
    result_file = output_dir / "actant_analysis_results.html"
    with open(result_file, 'w', encoding='utf-8') as f:
        f.write(html_content)
    print(f"âœ… åˆ†æç»“æœå·²ä¿å­˜åˆ°: {result_file}")


def main():
    """ä¸»å‡½æ•°"""
    print("="*80)
    print("ğŸš€ è¡ŒåŠ¨å…ƒåˆ†æ (Actant Analysis)")
    print("="*80)
    
    # 1. åŠ è½½æ–‡æ¡£ï¼ˆä»articles_txtç›®å½•ï¼‰
    texts, metadata = load_documents_from_txt_dir(ARTICLES_TXT_DIR)
    
    if not texts:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆæ–‡æ¡£ï¼Œé€€å‡º")
        return
    
    # 2. åˆ†æè¡ŒåŠ¨å…ƒ
    entities, actions, relations = analyze_actants(texts, metadata, OUTPUT_DIR)
    
    # 3. å¯è§†åŒ–
    visualize_actant_statistics(entities, actions, relations, metadata, OUTPUT_DIR)
    
    # 4. ä¿å­˜ç»“æœ
    save_actant_results(entities, actions, relations, OUTPUT_DIR)
    
    print("\n" + "="*80)
    print("âœ… åˆ†æå®Œæˆï¼")
    print(f"ğŸ“ ç»“æœä¿å­˜åœ¨: {OUTPUT_DIR}")
    print("="*80)


if __name__ == "__main__":
    main()
