#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è‡ªåŠ¨ç”Ÿæˆ country_mapping.json æ–‡ä»¶
æ ¹æ® articles_txt ç›®å½•ä¸­çš„æ–‡ä»¶åå’Œ links.txt ä¸­çš„ä¿¡æ¯
"""

import json
import re
from pathlib import Path
from collections import Counter

# é…ç½®è·¯å¾„
PROJECT_ROOT = Path(__file__).parent.parent
ARTICLES_TXT_DIR = PROJECT_ROOT / "corpus" / "articles_txt"
LINKS_FILE = PROJECT_ROOT / "links.txt"
OUTPUT_FILE = PROJECT_ROOT / "corpus" / "country_mapping.json"

# ä»æ–‡ä»¶åå‰ç¼€åˆ°å›½å®¶çš„æ˜ å°„ï¼ˆåŸºäº links.txtï¼‰
PREFIX_TO_COUNTRY = {
    'china_fmprc': 'China',
    'china_focac': 'China',
    'china_yidaiyilu': 'China',
    'russia_kremlin': 'Russia',
    'russia_government': 'Russia',
    'kazakhstan': 'Kazakhstan',
    'indonesia_president': 'Indonesia',
    'indonesia_mofa': 'Indonesia',
    'egypt': 'Egypt',
    'ethiopia': 'Ethiopia',
    'nigeria': 'Nigeria',
    'mongolia': 'Mongolia',
    'serbia': 'Serbia',
    'uzbekistan': 'Uzbekistan',
    'morocco': 'Morocco',
    'tanzania': 'Tanzania',
    'uganda': 'Uganda',
    'kenya': 'Kenya',
    'south_africa': 'South_Africa'
}


def normalize_country_name(country: str) -> str:
    """æ ‡å‡†åŒ–å›½å®¶åç§°ï¼ˆä¸­æ–‡è½¬è‹±æ–‡ï¼Œå¤„ç†ç‰¹æ®Šæ ¼å¼ï¼‰"""
    # å¤„ç† "ä¸­å›½-ä¸­éè®ºå›" è¿™ç§æƒ…å†µ
    if 'ä¸­å›½' in country or country == 'ä¸­å›½':
        return 'China'
    elif 'ä¿„ç½—æ–¯' in country or country == 'ä¿„ç½—æ–¯':
        return 'Russia'
    elif 'å“ˆè¨å…‹æ–¯å¦' in country:
        return 'Kazakhstan'
    elif 'å°åº¦å°¼è¥¿äºš' in country:
        return 'Indonesia'
    elif 'åŸƒåŠ' in country:
        return 'Egypt'
    elif 'åŸƒå¡ä¿„æ¯”äºš' in country:
        return 'Ethiopia'
    elif 'å°¼æ—¥åˆ©äºš' in country:
        return 'Nigeria'
    elif 'è’™å¤' in country:
        return 'Mongolia'
    elif 'å¡å°”ç»´äºš' in country:
        return 'Serbia'
    elif 'ä¹Œå…¹åˆ«å…‹æ–¯å¦' in country:
        return 'Uzbekistan'
    elif 'æ‘©æ´›å“¥' in country:
        return 'Morocco'
    elif 'å¦æ¡‘å°¼äºš' in country:
        return 'Tanzania'
    elif 'ä¹Œå¹²è¾¾' in country:
        return 'Uganda'
    elif 'è‚¯å°¼äºš' in country:
        return 'Kenya'
    elif 'å—é' in country or 'South Africa' in country:
        return 'South_Africa'
    
    # å¦‚æœå·²ç»æ˜¯è‹±æ–‡ï¼Œç›´æ¥è¿”å›ï¼ˆå¤„ç†ç©ºæ ¼å’Œä¸‹åˆ’çº¿ï¼‰
    return country.replace(' ', '_').replace('-', '_')


def parse_links_file(links_file: Path) -> dict:
    """ä» links.txt è§£ææ–‡ä»¶åå‰ç¼€åˆ°å›½å®¶çš„æ˜ å°„"""
    mapping = {}
    
    if not links_file.exists():
        print(f"âš ï¸  {links_file} ä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜è®¤æ˜ å°„")
        return {f"{prefix}_links.txt": country for prefix, country in PREFIX_TO_COUNTRY.items()}
    
    with open(links_file, 'r', encoding='utf-8') as f:
        for line in f:
            line = line.strip()
            # è·³è¿‡æ³¨é‡Šå’Œç©ºè¡Œ
            if not line or line.startswith('#') or '|' not in line:
                continue
            
            parts = [p.strip() for p in line.split('|')]
            if len(parts) >= 4:
                url = parts[0]
                country_raw = parts[1]
                keyword = parts[2]
                prefix = parts[3]
                
                # æ ‡å‡†åŒ–å›½å®¶åç§°
                country = normalize_country_name(country_raw)
                
                # ç”Ÿæˆæ–‡ä»¶åï¼š{prefix}_links.txt
                filename = f"{prefix}_links.txt"
                mapping[filename] = country
    
    return mapping


def extract_src_file_from_filename(filename: str) -> str:
    """
    ä»æ–‡ä»¶åæå– src_fileï¼ˆæ–‡ä»¶åå‰ç¼€ï¼‰
    
    æ–‡ä»¶åæ ¼å¼: {src_file}_{hash}_{title}.txt
    ä¾‹å¦‚: china_fmprc_links.txt_1f4fe6cc01f2c5b2__ä¸€å¸¦ä¸€è·¯_æ—¥æ–°æœˆå¼‚_ä¸­å¸Œåˆä½œåªäº‰æœå¤•_ä¸­åäººæ°‘å…±å’Œå›½å¤–äº¤éƒ¨.txt
    
    è¿”å›: src_file (ä¾‹å¦‚: "china_fmprc_links.txt")
    """
    # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åŒ¹é… {prefix}_links.txt æ¨¡å¼
    # åŒ¹é…æ ¼å¼: {ä»»æ„å­—ç¬¦}_links.txtï¼Œåé¢è·Ÿç€ä¸‹åˆ’çº¿å’Œå“ˆå¸Œå€¼
    pattern = r'^(.+?_links\.txt)_[a-f0-9]+'
    match = re.match(pattern, filename)
    if match:
        return match.group(1)
    
    # å¦‚æœæ­£åˆ™åŒ¹é…å¤±è´¥ï¼Œä½¿ç”¨åŸæ¥çš„æ–¹æ³•ä½œä¸ºåå¤‡
    parts = filename.split('_')
    for i in range(len(parts)):
        candidate = '_'.join(parts[:i+1])
        if 'links.txt' in candidate:
            return candidate
    
    return None


def extract_src_files_from_articles_txt(directory: Path) -> set:
    """ä» articles_txt ç›®å½•ä¸­çš„æ–‡ä»¶åæå–æ‰€æœ‰å”¯ä¸€çš„ src_file"""
    if not directory.exists():
        print(f"âš ï¸  ç›®å½• {directory} ä¸å­˜åœ¨")
        return set()
    
    src_files = set()
    
    for txt_file in directory.glob("*.txt"):
        src_file = extract_src_file_from_filename(txt_file.name)
        if src_file:
            src_files.add(src_file)
        else:
            print(f"âš ï¸  è­¦å‘Š: æ— æ³•ä»æ–‡ä»¶åæå– src_file: {txt_file.name}")
    
    return src_files


def count_documents_by_country(corpus_dir: Path, country_mapping: dict) -> dict:
    """ç»Ÿè®¡æ¯ä¸ªå›½å®¶çš„æ–‡æ¡£æ•°é‡"""
    if not corpus_dir.exists():
        return {}
    
    txt_dir = corpus_dir / "articles_txt"
    if not txt_dir.exists():
        return {}
    
    country_counts = Counter()
    unmatched_files = []
    
    for txt_file in txt_dir.glob("*.txt"):
        # ä»æ–‡ä»¶åæå– src_file
        src_file = extract_src_file_from_filename(txt_file.name)
        
        if src_file and src_file in country_mapping:
            country = country_mapping[src_file]
            country_counts[country] += 1
        else:
            unmatched_files.append(txt_file.name)
    
    if unmatched_files:
        print(f"âš ï¸  è­¦å‘Š: {len(unmatched_files)} ä¸ªæ–‡ä»¶æ— æ³•åŒ¹é…åˆ°å›½å®¶ï¼ˆå‰5ä¸ªç¤ºä¾‹ï¼‰:")
        for f in unmatched_files[:5]:
            print(f"     - {f}")
    
    return dict(country_counts)


def generate_country_mapping():
    """ç”Ÿæˆ country_mapping.json æ–‡ä»¶"""
    print("=" * 60)
    print("ç”Ÿæˆ country_mapping.json")
    print("=" * 60)
    
    # 1. ä» links.txt è§£ææ˜ å°„
    print("\nğŸ“– ä» links.txt è§£ææ˜ å°„...")
    links_mapping = parse_links_file(LINKS_FILE)
    print(f"   ä» links.txt è§£æåˆ° {len(links_mapping)} ä¸ªæ˜ å°„")
    
    # 2. ä» articles_txt ç›®å½•æå–å®é™…çš„ src_file
    print("\nğŸ“ ä» articles_txt ç›®å½•æå– src_file...")
    src_files = extract_src_files_from_articles_txt(ARTICLES_TXT_DIR)
    print(f"   æ‰¾åˆ° {len(src_files)} ä¸ªä¸åŒçš„ src_file")
    for sf in sorted(src_files):
        print(f"     - {sf}")
    
    # 3. åˆå¹¶æ˜ å°„ï¼ˆä¼˜å…ˆä½¿ç”¨ links.txt çš„æ˜ å°„ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨é»˜è®¤æ˜ å°„ï¼‰
    country_mapping = {}
    for src_file in src_files:
        if src_file in links_mapping:
            country_mapping[src_file] = links_mapping[src_file]
        else:
            # å°è¯•ä»æ–‡ä»¶åæ¨æ–­
            prefix = src_file.replace('_links.txt', '')
            if prefix in PREFIX_TO_COUNTRY:
                country_mapping[src_file] = PREFIX_TO_COUNTRY[prefix]
            else:
                print(f"âš ï¸  è­¦å‘Š: æ— æ³•ç¡®å®š {src_file} çš„å›½å®¶ï¼Œè·³è¿‡")
    
    print(f"\nâœ… ç”Ÿæˆ {len(country_mapping)} ä¸ªå›½å®¶æ˜ å°„")
    
    # 4. ç»Ÿè®¡æ–‡æ¡£æ•°é‡
    print("\nğŸ“Š ç»Ÿè®¡æ–‡æ¡£æ•°é‡...")
    corpus_dir = PROJECT_ROOT / "corpus"
    statistics = count_documents_by_country(corpus_dir, country_mapping)
    
    # 5. ç”Ÿæˆå®Œæ•´çš„ JSON æ•°æ®
    output_data = {
        "country_mapping": country_mapping,
        "statistics": statistics,
        "total_sources": len(country_mapping)
    }
    
    # 6. ä¿å­˜æ–‡ä»¶
    OUTPUT_FILE.parent.mkdir(parents=True, exist_ok=True)
    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
        json.dump(output_data, f, ensure_ascii=False, indent=2)
    
    print(f"\nâœ… å·²ä¿å­˜åˆ°: {OUTPUT_FILE}")
    print(f"\nğŸ“‹ æ˜ å°„è¯¦æƒ…:")
    for filename, country in sorted(country_mapping.items()):
        count = statistics.get(country, 0)
        print(f"   {filename:30s} -> {country:15s} ({count} ç¯‡æ–‡æ¡£)")
    
    print("\n" + "=" * 60)
    print("å®Œæˆï¼")
    print("=" * 60)


if __name__ == "__main__":
    generate_country_mapping()
